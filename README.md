# ‚ö° Junaid Ahmed Mohammed

### Senior Data Engineer | AI Infrastructure Architect
**Indianapolis, IN**

I am a **Senior Data Engineer (5+ YOE)** who builds the heavy-lifting infrastructure that makes AI possible. I specialize in bridging the gap between distributed systems (**Snowflake, Spark**) and modern Generative AI workflows (**Agentic AI, RAG**).

Currently, I focus on the intersection of **Enterprise Data Governance** and **Local LLM Inference**, optimizing billion-parameter models to run efficiently on constrained hardware.

---

### üß† The Philosophy
I believe the future of AI isn't just about bigger models‚Äîit's about **better pipelines**. 
* **Data Engineering:** Migrated **12TB+** legacy data to Snowflake Lakehouses, cutting infra costs by **25%**.
* **AI Research:** Fine-tuned **LLaMA 3.2 (3B)** and **Qwen 2.5** on 200k+ code samples using **QLoRA** and **Unsloth**.

---

### üõ†Ô∏è The Arsenal

| **Domain** | **Stack** |
| :--- | :--- |
| **Generative AI** | LLaMA 3.2, Qwen 2.5, LangChain, Hugging Face, Unsloth, QLoRA |
| **Cloud & Lakehouse** | Snowflake, Azure (ADF, Synapse), Databricks |
| **Streaming & Compute** | Apache Spark, PySpark, Kafka, Airflow |
| **Core Engineering** | Python, SQL, Scala, Docker, Kubernetes |

---

### üî≠ Current Focus
* **Agentic Workflows:** Exploring the **Model Context Protocol (MCP)** to give LLMs direct access to enterprise data lakes.
* **Quantization:** Experimenting with **4-bit quantization** techniques to maximize inference throughput.
* **Governance:** Implementing "Data-as-a-Product" principles to ensure GxP-compliant data integrity in AI pipelines.

---

### üí¨ Let's Talk About
* Why **Data Quality** is the bottleneck for GenAI (not compute).
* The trade-offs between **RAG** vs. **Long-Context Windows**.
* Optimizing **Snowflake** query performance for massive datasets.

[**LinkedIn**](https://www.linkedin.com/in/junaid-ahmed-mohammed-02321a95/) ‚Ä¢ [**Email**](mailto:mjunaidtechs@gmail.com)
